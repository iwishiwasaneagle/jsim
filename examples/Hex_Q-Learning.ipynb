{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hex Tiled Q-Learning SAR example\n",
    "\n",
    "The challenge in this example is implementing a Q-Learning search algorithm in a hexagonal tiled environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys, os\n",
    "from loguru import logger\n",
    "\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), \"../src\")) # run from within examples folder\n",
    "\n",
    "if os.environ.get(\"CI\", False):\n",
    "    logger.remove()\n",
    "    logger.add(sys.stderr, level=\"INFO\")\n",
    "\n",
    "logger.add(\"/tmp/ql.log\", level='DEBUG')\n",
    "\n",
    "BUILD: str = os.environ.get(\"BUILD\", \"\")\n",
    "num_trials: int\n",
    "steps: int\n",
    "if BUILD == \"doctest\": # tox env specific to doctests\n",
    "    num_trials = 1\n",
    "    steps = 5\n",
    "elif BUILD == \"html\": # tox env specific to building docs\n",
    "    num_trials = 30000\n",
    "    steps = 20\n",
    "else: # normal operation\n",
    "    num_trials = 500000\n",
    "    steps = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from copy import copy\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from jsim.Environment import HexEnvironment, HexDirections\n",
    "from jsim.Agent import Agent\n",
    "from jsim.Simulation import Simulation\n",
    "from jsim.Environment.HexEnvironment.HexCoords import OffsetCoord\n",
    "from jsim.Meta import State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class QHexEnv(HexEnvironment):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.pdm = self._generate_pdm()\n",
    "        self.n_bins = 4\n",
    "        self.digitized_pdm = self._encode_pdm(n=self.n_bins)\n",
    "        self._digitized_pdm_store = np.copy(self.digitized_pdm)\n",
    "\n",
    "        self._encode_penalty()\n",
    "\n",
    "        self._generate_states()\n",
    "\n",
    "    def _generate_pdm(self, N:int=5):\n",
    "        self._shape = (self._shape[0]+2, self._shape[1]+2)\n",
    "\n",
    "        x, y = np.meshgrid(np.arange(self.shape[0]), np.arange(self.shape[1]))\n",
    "\n",
    "        pdm = np.zeros(self.shape) # pad the PDM with zeros on all sides\n",
    "\n",
    "        for _ in range(N): # Generate N gaussians\n",
    "            A = max(np.random.rand()*2,1)\n",
    "            a = np.random.rand()*0.03\n",
    "            b = 0\n",
    "            c = np.random.rand()*0.03\n",
    "            x0 = np.random.uniform(0,self.shape[0]) # Centre of gaussian\n",
    "            y0 = np.random.uniform(0,self.shape[1])\n",
    "\n",
    "            pdm += A*np.exp(-(a*(x-x0)**2+2*b*(x-x0)*(y-y0)+c*(y-y0)**2))\n",
    "\n",
    "        return pdm\n",
    "\n",
    "    def _encode_penalty(self):\n",
    "        penalty = -1\n",
    "        self.pdm[0,:] = penalty \n",
    "        self.pdm[-1,:] = penalty\n",
    "        self.pdm[:,0] = penalty\n",
    "        self.pdm[:,-1] = penalty\n",
    "\n",
    "        self.digitized_pdm[0,:] = -1 \n",
    "        self.digitized_pdm[-1,:] = -1\n",
    "        self.digitized_pdm[:,0] = -1\n",
    "        self.digitized_pdm[:,-1] = -1\n",
    "\n",
    "    def _encode_pdm(self, n: int) -> np.ndarray:\n",
    "        n_bins = n\n",
    "        bins = np.arange(0,np.max(self.pdm), np.max(self.pdm)/n_bins)\n",
    "        return np.digitize(self.pdm, bins)\n",
    "\n",
    "    def _generate_states(self) -> None:\n",
    "        c = np.arange(-1,1+np.max(self.digitized_pdm))\n",
    "        dirs = np.array([f.flatten() for f in np.meshgrid(c,c,c,c,c,c)])\n",
    "        pstates = np.unique(dirs,axis=1)\n",
    "        i = -1\n",
    "        self.possible_states = {(a,b,c,d,e,f): (i:=i+1) for a,b,c,d,e,f in pstates.transpose()}\n",
    "\n",
    "        # Old method, used as a warning against not using numpy for everything\n",
    "        # This would take about 7s to run. The above method takes about 0.2s\n",
    "        #\n",
    "        # c = np.arange(-1,1+np.max(self.digitized_pdm))\n",
    "        # u,v,w,x,y,z = np.meshgrid(c,c,c,c,c,c) # one for each direction (6)\n",
    "        # u,v,w,x,y,z = u.flatten(), v.flatten(), w.flatten(), x.flatten(), y.flatten(), z.flatten()\n",
    "        # self.possible_states = {(a,b,c,d,e,f): int(np.where((u==a)&(v==b)&(w==c)&(x==d)&(y==e)&(z==f))[0]) for a,b,c,d,e,f in zip(u,v,w,x,y,z)}\n",
    "\n",
    "\n",
    "    def _state_from_list(self, l):\n",
    "        assert len(l) == len(HexDirections)\n",
    "        return self.possible_states[tuple(l)]\n",
    "\n",
    "    def _evaluate_digi_position(self, pos: OffsetCoord) -> int:\n",
    "        return self.digitized_pdm[pos.row, pos.col]\n",
    "    \n",
    "    def _evaluate_position(self, pos: OffsetCoord) -> float:\n",
    "        return self.pdm[pos.row, pos.col]\n",
    "\n",
    "    def reset(self, agent_p: OffsetCoord) -> int:\n",
    "        self.digitized_pdm = np.copy(self._digitized_pdm_store)\n",
    "        return self._evaluate_digi_position(agent_p)\n",
    "\n",
    "    def step(self, agent_s: OffsetCoord) -> tuple[int, float]:\n",
    "        reward = self._evaluate_position(agent_s)\n",
    "        neighbors = self.neighbors_coord(agent_s)\n",
    "\n",
    "        values_at_neighbors = self._state_from_list([self._evaluate_digi_position(n) for n in neighbors])\n",
    "\n",
    "        self.digitized_pdm[agent_s.row][agent_s.col] = -1\n",
    "\n",
    "        return values_at_neighbors, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class QAgent(Agent):\n",
    "    def __init__(self, penv: QHexEnv = None) -> None:\n",
    "        self.state = OffsetCoord(col=0,row=0)\n",
    "        self.states = []\n",
    "        self.penv = penv\n",
    "\n",
    "        self.epsilon = 1\n",
    "        self.qtable = np.zeros((len(self.penv.possible_states),6))\n",
    "\n",
    "    def policy(self, pnext_s: int) -> HexDirections:\n",
    "        epsilon = 0.2\n",
    "        result = None\n",
    "\n",
    "        if np.random.uniform(0,1) < epsilon:\n",
    "            idx = np.random.randint(0,6)\n",
    "        else:\n",
    "            idx = np.argmax(self.qtable[pnext_s])\n",
    "        result = HexDirections(idx)\n",
    "        return result\n",
    "\n",
    "    def step(self, pnext_s: int) -> HexDirections:\n",
    "        return self.policy(pnext_s)\n",
    "\n",
    "    def learn(self, reward: float, state: int, next_state: int,  action: HexDirections) -> None:\n",
    "        \"\"\"\n",
    "        Maximize the reward through a Q-Learning Step\n",
    "\n",
    "        :param reward: Reward based on the probability seen from the previous action by the agent\n",
    "        :type reward: float\n",
    "        :param state: The state of the digitized PDM around the agent when the previous action was deciced\n",
    "        :type state: int\n",
    "        :param next_state: The new state that the agent is in\n",
    "        :type next_state: int\n",
    "        :param action: The action by the agent\n",
    "        :type action: HexDirections\n",
    "        \"\"\"\n",
    "        alpha = 0.01\n",
    "        gamma = 0.99\n",
    "\n",
    "        reward *= -1\n",
    "\n",
    "        new_value = (1 - alpha) * self.qtable[state,action] + alpha * (reward+gamma*np.max(self.qtable[next_state]))\n",
    "\n",
    "        self.qtable[state][action] = new_value\n",
    "\n",
    "\n",
    "    def update(self, pa: HexDirections) -> OffsetCoord:\n",
    "        if not hasattr(self,'state'):\n",
    "            logger.error(f\"Reset has not been called as {hasattr(self,'state')=}\")\n",
    "            raise Exception\n",
    "\n",
    "        next_state = QHexEnv.neighbor_coord(self.state, pa)\n",
    "        self.state = next_state\n",
    "\n",
    "        return copy(self.state)\n",
    "\n",
    "    def reset(self, ps: OffsetCoord, vicinity: int) -> tuple[HexDirections, OffsetCoord]:\n",
    "        self.epsilon = np.max([0.01,self.epsilon*0.99])\n",
    "        self.state = copy(ps)\n",
    "        return self.policy(vicinity), self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class QSim(Simulation):\n",
    "    agent: QAgent\n",
    "    env: QHexEnv\n",
    "\n",
    "    def __init__(self, initial_pos=OffsetCoord(col=5,row=5)) -> None:\n",
    "        self.env = QHexEnv(psim=self)\n",
    "        self.agent = QAgent(penv=self.env)\n",
    "\n",
    "        self.initial_agent_s = initial_pos\n",
    "\n",
    "        self.data_store = {'env_s':[],'agent_s':[],'agent_a':[],'reward':[]}\n",
    "        self.long_term_ds = {}\n",
    "        super().__init__()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self.vicinity_pdm = self.env.reset(self.initial_agent_s)\n",
    "        self.agent_a, self.agent_s = self.agent.reset(copy(self.initial_agent_s),self.vicinity_pdm)\n",
    "        self.data_store = {'env_s':[],'agent_s':[],'agent_a':[],'reward':[]}\n",
    "\n",
    "    def trials(self, num_trials: int, max_num_steps: int):\n",
    "        for i in range(num_trials):\n",
    "            try:\n",
    "                self.steps(max_num_steps)\n",
    "                logger.debug(f\"Trial = {i}/{num_trials} | Max number of steps ({max_num_steps}) reached \")\n",
    "            except IndexError as e:\n",
    "                logger.debug(f\"Trial = {i}/{num_trials} | Agent went out of bounds, resetting\")\n",
    "            finally:\n",
    "                self.data_store['total_reward'] = np.sum(self.data_store['reward'])\n",
    "                logger.debug(f\"Trial = {i}/{num_trials} | Total reward = {self.data_store['total_reward']:.2f}\")\n",
    "                \n",
    "                # trim the data a bit to save memory\n",
    "                self.data_store.pop(\"agent_a\",None)\n",
    "                self.data_store.pop(\"env_s\",None)\n",
    "                self.data_store.pop(\"reward\",None)\n",
    "\n",
    "                self.long_term_ds[i] = copy(self.data_store)\n",
    "\n",
    "    def steps(self, num_steps: int) -> None:\n",
    "        self.reset()\n",
    "        \n",
    "        agent_s = self.agent_s\n",
    "        for _ in range(num_steps):\n",
    "            vicinity_pdm, reward = self.env.step(self.agent_s)\n",
    "\n",
    "            self.collect_data(vicinity_pdm, self.agent_a, self.agent_s, reward)\n",
    "\n",
    "            agent_a = self.agent.step(vicinity_pdm)\n",
    "\n",
    "            if agent_s and (agent_s.row > 0 or \\\n",
    "                    agent_s.col > 0 or \\\n",
    "                    agent_s.row < self.env.shape[0] or \\\n",
    "                    agent_s.col < self.env.shape[1]):\n",
    "\n",
    "                agent_s = self.agent.update(agent_a)\n",
    "            else:\n",
    "                print(agent_s, reward)\n",
    "\n",
    "            if agent_s.row < 0 or agent_s.col < 0:\n",
    "                raise IndexError(f\"Out of bounds with {agent_s} < 0\")\n",
    "            if agent_s.row > self.env.shape[0] or agent_s.col > self.env.shape[1]:\n",
    "                raise IndexError(f\"Out of bounds with {agent_s} > {self.env.shape}\")\n",
    "\n",
    "            self.agent.learn(reward, self.vicinity_pdm, vicinity_pdm, agent_a)\n",
    "\n",
    "            self.agent_s = agent_s\n",
    "            self.agent_a = agent_a\n",
    "            self.vicinity_pdm = vicinity_pdm\n",
    "\n",
    "\n",
    "\n",
    "    def collect_data(self, env_s: int, agent_a: HexDirections, agent_s: OffsetCoord, reward: float) -> None:\n",
    "        self.data_store['env_s'].append(copy(env_s))\n",
    "        self.data_store['agent_a'].append(copy(agent_a))\n",
    "        self.data_store['agent_s'].append(copy(agent_s))\n",
    "        self.data_store['reward'].append(copy(reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(sim: QSim):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(8, 6), dpi=80)\n",
    "\n",
    "    t = np.arange(0,len(sim.long_term_ds))\n",
    "\n",
    "    # Reward over time\n",
    "    ax1.plot(sim.long_term_ds.keys(), [sim.long_term_ds[ds]['total_reward'] for ds in sim.long_term_ds])\n",
    "    ax1.set_ylabel('Total reward')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "\n",
    "    # Show pdm and path\n",
    "    import operator\n",
    "    best_ds = max(sim.long_term_ds.values(), key=operator.itemgetter('total_reward'))\n",
    "    xy = [HexEnvironment.offset_to_pixel(f) for f in best_ds['agent_s']]\n",
    "    x = [f.x for f in xy]\n",
    "    y = [f.y for f in xy]\n",
    "\n",
    "    for p in sim.env.as_mpl_polygons(cmap=mpl.cm.get_cmap('gray')):\n",
    "        ax2.add_patch(p)\n",
    "\n",
    "    ax2.plot(x,y)\n",
    "    ax2.scatter(x[0],y[0],label='Start')\n",
    "    ax2.scatter(x[-1],y[-1],label='End')\n",
    "    ax2.set_ylabel('y')\n",
    "    ax2.set_xlabel('x')\n",
    "    ax2.legend()\n",
    "    ax2.set_aspect('equal')\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    logger.info(f\"Number of steps: {steps}\")\n",
    "    logger.info(f\"Number of trials: {num_trials}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Learn\n",
    "sim = QSim()\n",
    "\n",
    "sim.trials(num_trials,steps)\n",
    "\n",
    "plot(sim)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
